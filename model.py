import streamlit as st
import numpy as np
import pandas as pd
from sklearn import datasets

# Title and description
st.title("Machine Learning Algorithm Education")
st.markdown(
    """
    This page provides detailed information about various machine learning algorithms used in this application. 
    Select an algorithm to learn more about its characteristics, advantages, disadvantages, common use cases, 
    and an example implementation.
    """
)

# Algorithm selection
algorithm = st.selectbox(
    "Select an Algorithm to learn more:",
    [
        "Gaussian Naive Bayes (GaussianNB)",
        "Logistic Regression",
        "Random Forest",
        "SVM (RBF Kernel)",
        "k-Nearest Neighbors",
        "Decision Tree",
    ]
)

# Dictionary-based structure for details
algorithm_details = {
    "Gaussian Naive Bayes (GaussianNB)": {
        "Description": "A probabilistic classifier based on Bayes' theorem with strong independence assumptions. Assumes features follow a Gaussian (normal) distribution.",
        "Formula": r"P(y|X) = \frac{P(X|y) P(y)}{P(X)}",
        "Advantages": [
            "âœ… Simple and fast",
            "âœ… Works well with small datasets",
            "âœ… Good for high-dimensional data",
            "âœ… Performs well when features are normally distributed",
        ],
        "Disadvantages": [
            "âš ï¸ Assumes feature independence (often unrealistic)",
            "âš ï¸ Limited by Gaussian distribution assumption",
            "âš ï¸ May underperform when features are highly correlated",
        ],
        "Common Use Cases": [
            "ğŸ¯ Text classification",
            "ğŸ¯ Spam detection",
            "ğŸ¯ Medical diagnosis",
            "ğŸ¯ Real-time prediction scenarios",
        ],
        "Example Code": """
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = GaussianNB()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
    "Logistic Regression": {
        "Description": "A linear model for binary classification that predicts probabilities using the logistic function.",
        "Formula": r"f(x) = \frac{1}{1 + e^{-(w \cdot x + b)}}",
        "Advantages": [
            "âœ… Simple and interpretable",
            "âœ… Works well for linearly separable data",
            "âœ… Efficient for large datasets",
        ],
        "Disadvantages": [
            "âš ï¸ Assumes linear relationship between features and target",
            "âš ï¸ Sensitive to multicollinearity",
        ],
        "Common Use Cases": [
            "ğŸ¯ Binary classification",
            "ğŸ¯ Medical diagnosis",
            "ğŸ¯ Customer churn prediction",
        ],
        "Example Code": """
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics: accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X = X[y != 2]  # Binary classification
y = y[y != 2]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
    "Random Forest": {
        "Description": "An ensemble learning method that builds multiple decision trees and combines their outputs.",
        "Formula": r"F(x) = \frac{1}{T} \sum_{t=1}^{T} h_t(x)",
        "Advantages": [
            "âœ… Handles non-linear relationships",
            "âœ… Resistant to overfitting (with enough trees)",
            "âœ… Handles high-dimensional data",
        ],
        "Disadvantages": [
            "âš ï¸ Computationally expensive",
            "âš ï¸ Less interpretable than single decision trees",
        ],
        "Common Use Cases": [
            "ğŸ¯ Classification and regression tasks",
            "ğŸ¯ Feature importance analysis",
            "ğŸ¯ Fraud detection",
        ],
        "Example Code": """
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics: accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
    "SVM (RBF Kernel)": {
        "Description": "A powerful classifier that uses hyperplanes to separate data points in a high-dimensional space. The RBF kernel maps data points non-linearly to better handle complex relationships.",
        "Formula": r"K(x_i, x_j) = e^{-\gamma ||x_i - x_j||^2}",
        "Advantages": [
            "âœ… Effective for high-dimensional data",
            "âœ… Works well for non-linear boundaries",
        ],
        "Disadvantages": [
            "âš ï¸ Computationally intensive for large datasets",
            "âš ï¸ Requires careful parameter tuning (C and gamma)",
        ],
        "Common Use Cases": [
            "ğŸ¯ Image recognition",
            "ğŸ¯ Text categorization",
            "ğŸ¯ Bioinformatics",
        ],
        "Example Code": """
from sklearn.svm import SVC
from sklearn.model_selection: train_test_split
from sklearn.metrics: accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = SVC(kernel='rbf', C=1, gamma=0.1)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
    "k-Nearest Neighbors": {
        "Description": "A simple instance-based learning algorithm that predicts the label of a new data point based on the labels of its nearest neighbors.",
        "Formula": r"d(x, x') = \sqrt{\sum_{i=1}^{n} (x_i - x'_i)^2}",
        "Advantages": [
            "âœ… Simple and intuitive",
            "âœ… No training phase (lazy learning)",
        ],
        "Disadvantages": [
            "âš ï¸ Computationally expensive during prediction",
            "âš ï¸ Sensitive to noise and irrelevant features",
        ],
        "Common Use Cases": [
            "ğŸ¯ Pattern recognition",
            "ğŸ¯ Recommendation systems",
            "ğŸ¯ Medical diagnosis",
        ],
        "Example Code": """
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection: train_test_split
from sklearn.metrics: accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
    "Decision Tree": {
        "Description": "A tree-structured model that splits the data based on feature thresholds to make predictions.",
        "Formula": r"H(X) = -\sum_{i=1}^{n} p_i \log_2(p_i)",
        "Advantages": [
            "âœ… Easy to interpret",
            "âœ… Handles both categorical and numerical data",
            "âœ… Requires little data preprocessing",
        ],
        "Disadvantages": [
            "âš ï¸ Prone to overfitting (without pruning)",
            "âš ï¸ May create biased splits with imbalanced data",
        ],
        "Common Use Cases": [
            "ğŸ¯ Classification and regression tasks",
            "ğŸ¯ Feature selection",
            "ğŸ¯ Risk analysis",
        ],
        "Example Code": """
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection: train_test_split
from sklearn.metrics: accuracy_score

# Sample dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
        """,
    },
}

# Retrieve details for the selected algorithm
details = algorithm_details[algorithm]

# Display sections with subheaders
st.subheader("Description:")
st.markdown(details["Description"])

advantages, disadvantages, common_use_cases = st.columns(3)

with advantages:
    st.subheader("Advantages:")
    st.markdown("\n".join(f"- {adv}" for adv in details["Advantages"]))
with disadvantages:
    st.subheader("Disadvantages:")
    st.markdown("\n".join(f"- {disadv}" for disadv in details["Disadvantages"]))
with common_use_cases:
    st.subheader("Common Use Cases:")
    st.markdown("\n".join(f"- {use_case}" for use_case in details["Common Use Cases"]))

st.subheader("Formula:")
st.latex(details["Formula"])

st.subheader("Example Code:")
st.code(details["Example Code"], language="python")
